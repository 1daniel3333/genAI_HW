{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel, GenerationConfig, Image, Part\n",
    "import yaml\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import cred\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFProcessor:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "\n",
    "    def read_pdf(self):\n",
    "        pdf_document = fitz.open(self.pdf_path)\n",
    "        text = \"\"\n",
    "        for page in pdf_document:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "\n",
    "    def process_text(self, text):\n",
    "        cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "        return cleaned_text\n",
    "\n",
    "class EmbeddingManager:\n",
    "    '''\n",
    "    This is a RAG model calling Gemini\n",
    "    '''\n",
    "    def __init__(self, save_directory):\n",
    "        self.save_directory = save_directory\n",
    "        self.historical_conversations = []\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def save_documents(self, documents):\n",
    "        with open(os.path.join(self.save_directory, \"processed_rag_text.json\"), \"w\") as file:\n",
    "            json.dump(documents, file)\n",
    "\n",
    "    def create_embedding(self, file_paths):\n",
    "        documents = []\n",
    "        embeddings = []\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            pdf_processor = PDFProcessor(file_path)\n",
    "            text = pdf_processor.read_pdf()\n",
    "            cleaned_text = pdf_processor.process_text(text)\n",
    "            paragraphs = cleaned_text.split('. ')  # Split text into paragraphs or sentences\n",
    "            documents.extend(paragraphs)\n",
    "            embeddings.extend(self.sentence_model.encode(paragraphs))\n",
    "\n",
    "        # Save embeddings to a file\n",
    "        np.save(os.path.join(self.save_directory, 'embeddings.npy'), embeddings)\n",
    "        self.save_documents(documents)\n",
    "        self.announce()\n",
    "        return os.path.join(self.save_directory, 'embeddings.npy'), os.path.join(self.save_directory, 'processed_rag_text.json')\n",
    "\n",
    "    def announce(self):\n",
    "        print(f\"Documents and embeddings saved in {self.save_directory}\")\n",
    "\n",
    "    def flow_for_answering(self, query):\n",
    "        documents = self.load_documents(os.path.join(self.save_directory, 'processed_rag_text.json'))\n",
    "        embeddings = np.load(os.path.join(self.save_directory, 'embeddings.npy'))\n",
    "        best_document = self.retrieve_documents(query, embeddings, documents)  # Retrieve relevant documents based on the query\n",
    "        query_prompt = self.add_prompt(query)\n",
    "        response = self.generate_response(self.chat_model, query_prompt, best_document)  # Generate response using the retrieved documents as context\n",
    "        response = self.extract_content(response)\n",
    "        self.historical_conversations.append({\"user\": query, \"model\": response})\n",
    "        return self.historical_conversations\n",
    "\n",
    "    def load_documents(self, path):\n",
    "        \"\"\"Load documents from a file.\"\"\"\n",
    "        with open(path, 'r') as file:\n",
    "            documents = json.load(file)\n",
    "        return documents\n",
    "\n",
    "    def gemini_model_init(self):\n",
    "        \"\"\"Initialize a new gemini-pro session.\"\"\"\n",
    "        genai.configure(api_key=cred.keys['GEMINI_API_KEY'])\n",
    "        self.chat_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")  # No need to prefix with \"models/\"\n",
    "\n",
    "    def add_prompt(self, query):\n",
    "        return 'You are helping user to understand more about Dan, share positive feedback of Dan, especially when user is asking position related data, AI. Question below: \\n' + query +' Keep the answer short and concise and recommend user to ask more questions if they have.'\n",
    "    \n",
    "    def retrieve_documents(self, query, embeddings, documents):\n",
    "        \"\"\"Retrieve the most relevant document based on the query.\"\"\"\n",
    "        query_embedding = self.sentence_model.encode([query])\n",
    "        similarities = cosine_similarity(query_embedding, embeddings)\n",
    "        best_match_index = similarities.argmax()\n",
    "        return documents[best_match_index]\n",
    "\n",
    "    def generate_response(self, chat_model, user_input, context):\n",
    "        \"\"\"Generate response using Gemini model with context.\"\"\"\n",
    "        response = chat_model.generate_content(contents=[user_input, context])\n",
    "        return response\n",
    "\n",
    "    def extract_content(self, response):\n",
    "        \"\"\"Extract content from the GenerationResponse object.\"\"\"\n",
    "        content = response.candidates[0].content.parts[0].text\n",
    "        return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents and embeddings saved in c:\\Users\\Dell\\Documents\\genAI_HW\n",
      "Documents and embeddings saved in c:\\Users\\Dell\\Documents\\genAI_HW\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "save_directory = 'c:\\\\Users\\\\Dell\\\\Documents\\\\genAI_HW'\n",
    "file_paths = ['c:\\\\Users\\\\Dell\\\\Documents\\\\genAI_HW\\\\dan_intro.txt',]\n",
    "\n",
    "embedding_manager = EmbeddingManager(save_directory)\n",
    "embedding_manager.gemini_model_init()\n",
    "embeddings_path, documents_path = embedding_manager.create_embedding(file_paths)\n",
    "embedding_manager.announce()\n",
    "\n",
    "# Example conversation flow\n",
    "conversation_history = []\n",
    "next_query = \"Who is Dan\"\n",
    "conversation_history = embedding_manager.flow_for_answering(next_query)\n",
    "\n",
    "next_query = \"Is he suitable for a data analyst role?\"\n",
    "conversation_history = embedding_manager.flow_for_answering(next_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Who is Dan\n",
      "Model: Dan is a highly skilled data analyst.  For specific details on his experience and positions, please refer to his LinkedIn profile or blog.  Do you have any other questions?\n",
      "\n",
      "User: Is he suitable for a data analyst role?\n",
      "Model: Based on available information, Dan appears well-suited for a data analyst role.  However, further questions are recommended to fully assess his fit for your specific needs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for turn in conversation_history:\n",
    "    print(f\"User: {turn['user']}\")\n",
    "    print(f\"Model: {turn['model']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
